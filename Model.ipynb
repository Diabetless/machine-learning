{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4VXm60U4RSN"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGwUjwgsnlIs"
      },
      "source": [
        "## Download Dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arQfHRxPnO0i",
        "outputId": "fe221e9d-7a79-4729-b8ef-02f6d192eb57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=186HlpFc60T0jWYrJPodAJNtYK6Hgr2tg\n",
            "To: /content/datasets.zip\n",
            "100% 187M/187M [00:01<00:00, 109MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 186HlpFc60T0jWYrJPodAJNtYK6Hgr2tg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2Bg3-UEhWK8",
        "outputId": "8c9851d2-1f29-445e-a67e-f0423f9379c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1blF8nj8UZ1Hq-UVs8TP1fsQ3WfheaNYq\n",
            "To: /content/data.yaml\n",
            "\r  0% 0.00/244 [00:00<?, ?B/s]\r100% 244/244 [00:00<00:00, 1.16MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1blF8nj8UZ1Hq-UVs8TP1fsQ3WfheaNYq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7Ts8aVZn2xw"
      },
      "source": [
        "## Unzip dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgmU6-jjn4XE"
      },
      "outputs": [],
      "source": [
        "!unzip -q datasets.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t63dV764GXk"
      },
      "source": [
        "## Remove Zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwWY9ypioGth"
      },
      "outputs": [],
      "source": [
        "!rm datasets.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwU8mQxQ4Za8"
      },
      "source": [
        "# Install Required Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvcd2ZB139z8",
        "outputId": "2b803153-20ba-490a-c413-21a17a1707ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q ultralytics\n",
        "!pip install onnx-tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vae28lfa4lrO"
      },
      "source": [
        "# Import Necessary Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPIcDkdQ4B8p"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "import onnx\n",
        "from onnx_tf.backend import prepare\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFZ9p-144t_v"
      },
      "source": [
        "# Instantiate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otwLWp2Z4EVX",
        "outputId": "14a7e01f-8dbe-4cdb-fe33-2cb75be36bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "layer                                     name  gradient   parameters                shape         mu      sigma\n",
            "    0                      model.0.conv.weight     False         1296        [48, 3, 3, 3]    -0.0012      0.112 torch.float32\n",
            "    1                        model.0.bn.weight     False           48                 [48]       3.25      0.805 torch.float32\n",
            "    2                          model.0.bn.bias     False           48                 [48]    -0.0398       2.34 torch.float32\n",
            "    3                      model.1.conv.weight     False        41472       [96, 48, 3, 3]  -0.000434     0.0234 torch.float32\n",
            "    4                        model.1.bn.weight     False           96                 [96]       2.95      0.632 torch.float32\n",
            "    5                          model.1.bn.bias     False           96                 [96]      0.457        1.5 torch.float32\n",
            "    6                  model.2.cv1.conv.weight     False         9216       [96, 96, 1, 1]   -0.00354     0.0362 torch.float32\n",
            "    7                    model.2.cv1.bn.weight     False           96                 [96]       1.59      0.818 torch.float32\n",
            "    8                      model.2.cv1.bn.bias     False           96                 [96]      0.457       1.17 torch.float32\n",
            "    9                  model.2.cv2.conv.weight     False        18432      [96, 192, 1, 1]   -0.00224     0.0288 torch.float32\n",
            "   10                    model.2.cv2.bn.weight     False           96                 [96]      0.983      0.302 torch.float32\n",
            "   11                      model.2.cv2.bn.bias     False           96                 [96]     -0.235      0.795 torch.float32\n",
            "   12              model.2.m.0.cv1.conv.weight     False        20736       [48, 48, 3, 3]  -0.000701     0.0228 torch.float32\n",
            "   13                model.2.m.0.cv1.bn.weight     False           48                 [48]       1.48      0.467 torch.float32\n",
            "   14                  model.2.m.0.cv1.bn.bias     False           48                 [48]      0.644       1.17 torch.float32\n",
            "   15              model.2.m.0.cv2.conv.weight     False        20736       [48, 48, 3, 3]   -0.00107     0.0208 torch.float32\n",
            "   16                model.2.m.0.cv2.bn.weight     False           48                 [48]       1.19      0.319 torch.float32\n",
            "   17                  model.2.m.0.cv2.bn.bias     False           48                 [48]      0.374       1.17 torch.float32\n",
            "   18              model.2.m.1.cv1.conv.weight     False        20736       [48, 48, 3, 3]  -0.000852     0.0191 torch.float32\n",
            "   19                model.2.m.1.cv1.bn.weight     False           48                 [48]       1.11      0.239 torch.float32\n",
            "   20                  model.2.m.1.cv1.bn.bias     False           48                 [48]     -0.101      0.961 torch.float32\n",
            "   21              model.2.m.1.cv2.conv.weight     False        20736       [48, 48, 3, 3]   -0.00122     0.0197 torch.float32\n",
            "   22                model.2.m.1.cv2.bn.weight     False           48                 [48]       1.91      0.405 torch.float32\n",
            "   23                  model.2.m.1.cv2.bn.bias     False           48                 [48]      0.493       1.06 torch.float32\n",
            "   24                      model.3.conv.weight     False       165888      [192, 96, 3, 3]  -0.000534     0.0122 torch.float32\n",
            "   25                        model.3.bn.weight     False          192                [192]       0.71      0.176 torch.float32\n",
            "   26                          model.3.bn.bias     False          192                [192]     -0.392       0.87 torch.float32\n",
            "   27                  model.4.cv1.conv.weight     False        36864     [192, 192, 1, 1]  -0.000807     0.0207 torch.float32\n",
            "   28                    model.4.cv1.bn.weight     False          192                [192]      0.924      0.467 torch.float32\n",
            "   29                      model.4.cv1.bn.bias     False          192                [192]      0.269      0.807 torch.float32\n",
            "   30                  model.4.cv2.conv.weight     False       110592     [192, 576, 1, 1]  -0.000771     0.0141 torch.float32\n",
            "   31                    model.4.cv2.bn.weight     False          192                [192]      0.918      0.209 torch.float32\n",
            "   32                      model.4.cv2.bn.bias     False          192                [192]     -0.598      0.816 torch.float32\n",
            "   33              model.4.m.0.cv1.conv.weight     False        82944       [96, 96, 3, 3]  -0.000716      0.011 torch.float32\n",
            "   34                model.4.m.0.cv1.bn.weight     False           96                 [96]      0.893       0.15 torch.float32\n",
            "   35                  model.4.m.0.cv1.bn.bias     False           96                 [96]      -0.78      0.696 torch.float32\n",
            "   36              model.4.m.0.cv2.conv.weight     False        82944       [96, 96, 3, 3]  -0.000411      0.011 torch.float32\n",
            "   37                model.4.m.0.cv2.bn.weight     False           96                 [96]      0.616      0.171 torch.float32\n",
            "   38                  model.4.m.0.cv2.bn.bias     False           96                 [96]     -0.191      0.706 torch.float32\n",
            "   39              model.4.m.1.cv1.conv.weight     False        82944       [96, 96, 3, 3]  -0.000814      0.011 torch.float32\n",
            "   40                model.4.m.1.cv1.bn.weight     False           96                 [96]      0.865      0.137 torch.float32\n",
            "   41                  model.4.m.1.cv1.bn.bias     False           96                 [96]       -1.2      0.668 torch.float32\n",
            "   42              model.4.m.1.cv2.conv.weight     False        82944       [96, 96, 3, 3]  -0.000566     0.0103 torch.float32\n",
            "   43                model.4.m.1.cv2.bn.weight     False           96                 [96]      0.703      0.199 torch.float32\n",
            "   44                  model.4.m.1.cv2.bn.bias     False           96                 [96]      -0.25      0.637 torch.float32\n",
            "   45              model.4.m.2.cv1.conv.weight     False        82944       [96, 96, 3, 3]  -0.000728      0.011 torch.float32\n",
            "   46                model.4.m.2.cv1.bn.weight     False           96                 [96]      0.767     0.0954 torch.float32\n",
            "   47                  model.4.m.2.cv1.bn.bias     False           96                 [96]      -1.44      0.566 torch.float32\n",
            "   48              model.4.m.2.cv2.conv.weight     False        82944       [96, 96, 3, 3]  -0.000428     0.0101 torch.float32\n",
            "   49                model.4.m.2.cv2.bn.weight     False           96                 [96]      0.869      0.166 torch.float32\n",
            "   50                  model.4.m.2.cv2.bn.bias     False           96                 [96]      -0.29      0.616 torch.float32\n",
            "   51              model.4.m.3.cv1.conv.weight     False        82944       [96, 96, 3, 3]  -0.000657     0.0108 torch.float32\n",
            "   52                model.4.m.3.cv1.bn.weight     False           96                 [96]      0.751      0.109 torch.float32\n",
            "   53                  model.4.m.3.cv1.bn.bias     False           96                 [96]      -1.28      0.583 torch.float32\n",
            "   54              model.4.m.3.cv2.conv.weight     False        82944       [96, 96, 3, 3]  -0.000452    0.00979 torch.float32\n",
            "   55                model.4.m.3.cv2.bn.weight     False           96                 [96]        1.4      0.244 torch.float32\n",
            "   56                  model.4.m.3.cv2.bn.bias     False           96                 [96]     0.0873      0.681 torch.float32\n",
            "   57                      model.5.conv.weight     False       663552     [384, 192, 3, 3]   -4.8e-05      0.007 torch.float32\n",
            "   58                        model.5.bn.weight     False          384                [384]      0.911      0.142 torch.float32\n",
            "   59                          model.5.bn.bias     False          384                [384]     -0.526      0.653 torch.float32\n",
            "   60                  model.6.cv1.conv.weight     False       147456     [384, 384, 1, 1]  -0.000872     0.0123 torch.float32\n",
            "   61                    model.6.cv1.bn.weight     False          384                [384]       1.12      0.478 torch.float32\n",
            "   62                      model.6.cv1.bn.bias     False          384                [384]     -0.171      0.788 torch.float32\n",
            "   63                  model.6.cv2.conv.weight     False       442368    [384, 1152, 1, 1]  -0.000461    0.00852 torch.float32\n",
            "   64                    model.6.cv2.bn.weight     False          384                [384]       1.05      0.141 torch.float32\n",
            "   65                      model.6.cv2.bn.bias     False          384                [384]     -0.957       0.75 torch.float32\n",
            "   66              model.6.m.0.cv1.conv.weight     False       331776     [192, 192, 3, 3]  -0.000467    0.00634 torch.float32\n",
            "   67                model.6.m.0.cv1.bn.weight     False          192                [192]       1.04      0.123 torch.float32\n",
            "   68                  model.6.m.0.cv1.bn.bias     False          192                [192]      -0.99      0.527 torch.float32\n",
            "   69              model.6.m.0.cv2.conv.weight     False       331776     [192, 192, 3, 3]  -0.000358     0.0065 torch.float32\n",
            "   70                model.6.m.0.cv2.bn.weight     False          192                [192]      0.747      0.153 torch.float32\n",
            "   71                  model.6.m.0.cv2.bn.bias     False          192                [192]     -0.492       0.57 torch.float32\n",
            "   72              model.6.m.1.cv1.conv.weight     False       331776     [192, 192, 3, 3]  -0.000462    0.00654 torch.float32\n",
            "   73                model.6.m.1.cv1.bn.weight     False          192                [192]      0.968     0.0956 torch.float32\n",
            "   74                  model.6.m.1.cv1.bn.bias     False          192                [192]      -1.24      0.561 torch.float32\n",
            "   75              model.6.m.1.cv2.conv.weight     False       331776     [192, 192, 3, 3]   -0.00023    0.00632 torch.float32\n",
            "   76                model.6.m.1.cv2.bn.weight     False          192                [192]      0.841      0.134 torch.float32\n",
            "   77                  model.6.m.1.cv2.bn.bias     False          192                [192]     -0.583      0.594 torch.float32\n",
            "   78              model.6.m.2.cv1.conv.weight     False       331776     [192, 192, 3, 3]  -0.000457    0.00667 torch.float32\n",
            "   79                model.6.m.2.cv1.bn.weight     False          192                [192]      0.939     0.0835 torch.float32\n",
            "   80                  model.6.m.2.cv1.bn.bias     False          192                [192]      -1.36      0.558 torch.float32\n",
            "   81              model.6.m.2.cv2.conv.weight     False       331776     [192, 192, 3, 3]  -0.000269     0.0063 torch.float32\n",
            "   82                model.6.m.2.cv2.bn.weight     False          192                [192]          1      0.138 torch.float32\n",
            "   83                  model.6.m.2.cv2.bn.bias     False          192                [192]     -0.596       0.56 torch.float32\n",
            "   84              model.6.m.3.cv1.conv.weight     False       331776     [192, 192, 3, 3]   -0.00047    0.00666 torch.float32\n",
            "   85                model.6.m.3.cv1.bn.weight     False          192                [192]      0.924     0.0729 torch.float32\n",
            "   86                  model.6.m.3.cv1.bn.bias     False          192                [192]      -1.39       0.53 torch.float32\n",
            "   87              model.6.m.3.cv2.conv.weight     False       331776     [192, 192, 3, 3]   -0.00027    0.00608 torch.float32\n",
            "   88                model.6.m.3.cv2.bn.weight     False          192                [192]        1.5      0.109 torch.float32\n",
            "   89                  model.6.m.3.cv2.bn.bias     False          192                [192]     -0.419      0.677 torch.float32\n",
            "   90                      model.7.conv.weight     False  1.99066e+06     [576, 384, 3, 3]  -0.000191    0.00419 torch.float32\n",
            "   91                        model.7.bn.weight     False          576                [576]       1.08      0.107 torch.float32\n",
            "   92                          model.7.bn.bias     False          576                [576]     -0.748      0.645 torch.float32\n",
            "   93                  model.8.cv1.conv.weight     False       331776     [576, 576, 1, 1]  -0.000892    0.00782 torch.float32\n",
            "   94                    model.8.cv1.bn.weight     False          576                [576]       1.11       0.18 torch.float32\n",
            "   95                      model.8.cv1.bn.bias     False          576                [576]     -0.539      0.901 torch.float32\n",
            "   96                  model.8.cv2.conv.weight     False       663552    [576, 1152, 1, 1]   -0.00053    0.00574 torch.float32\n",
            "   97                    model.8.cv2.bn.weight     False          576                [576]        1.2      0.126 torch.float32\n",
            "   98                      model.8.cv2.bn.bias     False          576                [576]     -0.441      0.769 torch.float32\n",
            "   99              model.8.m.0.cv1.conv.weight     False       746496     [288, 288, 3, 3]  -0.000328    0.00456 torch.float32\n",
            "  100                model.8.m.0.cv1.bn.weight     False          288                [288]       1.23       0.12 torch.float32\n",
            "  101                  model.8.m.0.cv1.bn.bias     False          288                [288]     -0.695      0.682 torch.float32\n",
            "  102              model.8.m.0.cv2.conv.weight     False       746496     [288, 288, 3, 3]   -0.00037    0.00443 torch.float32\n",
            "  103                model.8.m.0.cv2.bn.weight     False          288                [288]       1.06      0.162 torch.float32\n",
            "  104                  model.8.m.0.cv2.bn.bias     False          288                [288]     -0.714      0.715 torch.float32\n",
            "  105              model.8.m.1.cv1.conv.weight     False       746496     [288, 288, 3, 3]  -0.000279     0.0042 torch.float32\n",
            "  106                model.8.m.1.cv1.bn.weight     False          288                [288]        1.2       0.12 torch.float32\n",
            "  107                  model.8.m.1.cv1.bn.bias     False          288                [288]     -0.785        0.7 torch.float32\n",
            "  108              model.8.m.1.cv2.conv.weight     False       746496     [288, 288, 3, 3]  -0.000292    0.00401 torch.float32\n",
            "  109                model.8.m.1.cv2.bn.weight     False          288                [288]       1.43      0.186 torch.float32\n",
            "  110                  model.8.m.1.cv2.bn.bias     False          288                [288]     -0.374      0.673 torch.float32\n",
            "  111                  model.9.cv1.conv.weight     False       165888     [288, 576, 1, 1]   -0.00161    0.00961 torch.float32\n",
            "  112                    model.9.cv1.bn.weight     False          288                [288]      0.926      0.164 torch.float32\n",
            "  113                      model.9.cv1.bn.bias     False          288                [288]        0.5      0.812 torch.float32\n",
            "  114                  model.9.cv2.conv.weight     False       663552    [576, 1152, 1, 1]  -1.19e-05    0.00647 torch.float32\n",
            "  115                    model.9.cv2.bn.weight     False          576                [576]      0.935      0.142 torch.float32\n",
            "  116                      model.9.cv2.bn.bias     False          576                [576]      -1.19      0.813 torch.float32\n",
            "  117                 model.12.cv1.conv.weight     False       368640     [384, 960, 1, 1]   -0.00084     0.0103 torch.float32\n",
            "  118                   model.12.cv1.bn.weight     False          384                [384]       1.05      0.176 torch.float32\n",
            "  119                     model.12.cv1.bn.bias     False          384                [384]     -0.838      0.964 torch.float32\n",
            "  120                 model.12.cv2.conv.weight     False       294912     [384, 768, 1, 1]  -0.000953    0.00977 torch.float32\n",
            "  121                   model.12.cv2.bn.weight     False          384                [384]      0.864      0.146 torch.float32\n",
            "  122                     model.12.cv2.bn.bias     False          384                [384]      -0.78      0.906 torch.float32\n",
            "  123             model.12.m.0.cv1.conv.weight     False       331776     [192, 192, 3, 3]  -0.000573    0.00747 torch.float32\n",
            "  124               model.12.m.0.cv1.bn.weight     False          192                [192]       1.11      0.122 torch.float32\n",
            "  125                 model.12.m.0.cv1.bn.bias     False          192                [192]     -0.886      0.804 torch.float32\n",
            "  126             model.12.m.0.cv2.conv.weight     False       331776     [192, 192, 3, 3]   -0.00055    0.00719 torch.float32\n",
            "  127               model.12.m.0.cv2.bn.weight     False          192                [192]       0.95      0.119 torch.float32\n",
            "  128                 model.12.m.0.cv2.bn.bias     False          192                [192]     -0.886      0.779 torch.float32\n",
            "  129             model.12.m.1.cv1.conv.weight     False       331776     [192, 192, 3, 3]  -0.000607    0.00613 torch.float32\n",
            "  130               model.12.m.1.cv1.bn.weight     False          192                [192]       1.07      0.137 torch.float32\n",
            "  131                 model.12.m.1.cv1.bn.bias     False          192                [192]     -0.812      0.659 torch.float32\n",
            "  132             model.12.m.1.cv2.conv.weight     False       331776     [192, 192, 3, 3]  -0.000387    0.00584 torch.float32\n",
            "  133               model.12.m.1.cv2.bn.weight     False          192                [192]      0.962      0.115 torch.float32\n",
            "  134                 model.12.m.1.cv2.bn.bias     False          192                [192]     -0.736      0.622 torch.float32\n",
            "  135                 model.15.cv1.conv.weight     False       110592     [192, 576, 1, 1]  -0.000638     0.0131 torch.float32\n",
            "  136                   model.15.cv1.bn.weight     False          192                [192]      0.664      0.189 torch.float32\n",
            "  137                     model.15.cv1.bn.bias     False          192                [192]     -0.195       1.14 torch.float32\n",
            "  138                 model.15.cv2.conv.weight     False        73728     [192, 384, 1, 1]   -0.00079     0.0122 torch.float32\n",
            "  139                   model.15.cv2.bn.weight     False          192                [192]      0.588      0.252 torch.float32\n",
            "  140                     model.15.cv2.bn.bias     False          192                [192]     -0.522      0.936 torch.float32\n",
            "  141             model.15.m.0.cv1.conv.weight     False        82944       [96, 96, 3, 3]  -0.000674     0.0122 torch.float32\n",
            "  142               model.15.m.0.cv1.bn.weight     False           96                 [96]      0.824      0.113 torch.float32\n",
            "  143                 model.15.m.0.cv1.bn.bias     False           96                 [96]     -0.694      0.793 torch.float32\n",
            "  144             model.15.m.0.cv2.conv.weight     False        82944       [96, 96, 3, 3]  -0.000875     0.0114 torch.float32\n",
            "  145               model.15.m.0.cv2.bn.weight     False           96                 [96]      0.837      0.127 torch.float32\n",
            "  146                 model.15.m.0.cv2.bn.bias     False           96                 [96]     -0.611      0.792 torch.float32\n",
            "  147             model.15.m.1.cv1.conv.weight     False        82944       [96, 96, 3, 3]   -0.00102    0.00998 torch.float32\n",
            "  148               model.15.m.1.cv1.bn.weight     False           96                 [96]      0.811      0.154 torch.float32\n",
            "  149                 model.15.m.1.cv1.bn.bias     False           96                 [96]     -0.739      0.705 torch.float32\n",
            "  150             model.15.m.1.cv2.conv.weight     False        82944       [96, 96, 3, 3]  -0.000596    0.00925 torch.float32\n",
            "  151               model.15.m.1.cv2.bn.weight     False           96                 [96]      0.965      0.255 torch.float32\n",
            "  152                 model.15.m.1.cv2.bn.bias     False           96                 [96]     -0.203      0.633 torch.float32\n",
            "  153                     model.16.conv.weight     False       331776     [192, 192, 3, 3]  -0.000217     0.0048 torch.float32\n",
            "  154                       model.16.bn.weight     False          192                [192]      0.864      0.159 torch.float32\n",
            "  155                         model.16.bn.bias     False          192                [192]     -0.375      0.656 torch.float32\n",
            "  156                 model.18.cv1.conv.weight     False       221184     [384, 576, 1, 1]  -0.000512    0.00779 torch.float32\n",
            "  157                   model.18.cv1.bn.weight     False          384                [384]      0.993      0.127 torch.float32\n",
            "  158                     model.18.cv1.bn.bias     False          384                [384]     -0.496      0.804 torch.float32\n",
            "  159                 model.18.cv2.conv.weight     False       294912     [384, 768, 1, 1]  -0.000417    0.00618 torch.float32\n",
            "  160                   model.18.cv2.bn.weight     False          384                [384]      0.934      0.278 torch.float32\n",
            "  161                     model.18.cv2.bn.bias     False          384                [384]     -0.611      0.708 torch.float32\n",
            "  162             model.18.m.0.cv1.conv.weight     False       331776     [192, 192, 3, 3]  -0.000399    0.00585 torch.float32\n",
            "  163               model.18.m.0.cv1.bn.weight     False          192                [192]      0.988       0.16 torch.float32\n",
            "  164                 model.18.m.0.cv1.bn.bias     False          192                [192]     -0.742       0.64 torch.float32\n",
            "  165             model.18.m.0.cv2.conv.weight     False       331776     [192, 192, 3, 3]  -0.000333    0.00545 torch.float32\n",
            "  166               model.18.m.0.cv2.bn.weight     False          192                [192]       1.02      0.123 torch.float32\n",
            "  167                 model.18.m.0.cv2.bn.bias     False          192                [192]     -0.601      0.632 torch.float32\n",
            "  168             model.18.m.1.cv1.conv.weight     False       331776     [192, 192, 3, 3]  -0.000451    0.00481 torch.float32\n",
            "  169               model.18.m.1.cv1.bn.weight     False          192                [192]      0.977      0.155 torch.float32\n",
            "  170                 model.18.m.1.cv1.bn.bias     False          192                [192]     -0.704      0.556 torch.float32\n",
            "  171             model.18.m.1.cv2.conv.weight     False       331776     [192, 192, 3, 3]   -0.00026    0.00466 torch.float32\n",
            "  172               model.18.m.1.cv2.bn.weight     False          192                [192]       1.28       0.17 torch.float32\n",
            "  173                 model.18.m.1.cv2.bn.bias     False          192                [192]     -0.385      0.616 torch.float32\n",
            "  174                     model.19.conv.weight     False   1.3271e+06     [384, 384, 3, 3]  -0.000111    0.00235 torch.float32\n",
            "  175                       model.19.bn.weight     False          384                [384]      0.913      0.104 torch.float32\n",
            "  176                         model.19.bn.bias     False          384                [384]       -0.3      0.661 torch.float32\n",
            "  177                 model.21.cv1.conv.weight     False       552960     [576, 960, 1, 1]  -0.000278    0.00417 torch.float32\n",
            "  178                   model.21.cv1.bn.weight     False          576                [576]       1.03      0.115 torch.float32\n",
            "  179                     model.21.cv1.bn.bias     False          576                [576]     -0.386      0.836 torch.float32\n",
            "  180                 model.21.cv2.conv.weight     False       663552    [576, 1152, 1, 1]  -0.000232    0.00339 torch.float32\n",
            "  181                   model.21.cv2.bn.weight     False          576                [576]        1.1      0.193 torch.float32\n",
            "  182                     model.21.cv2.bn.bias     False          576                [576]     -0.416      0.684 torch.float32\n",
            "  183             model.21.m.0.cv1.conv.weight     False       746496     [288, 288, 3, 3]  -0.000196    0.00307 torch.float32\n",
            "  184               model.21.m.0.cv1.bn.weight     False          288                [288]       1.11      0.119 torch.float32\n",
            "  185                 model.21.m.0.cv1.bn.bias     False          288                [288]     -0.389      0.573 torch.float32\n",
            "  186             model.21.m.0.cv2.conv.weight     False       746496     [288, 288, 3, 3]  -0.000235    0.00299 torch.float32\n",
            "  187               model.21.m.0.cv2.bn.weight     False          288                [288]       1.05      0.117 torch.float32\n",
            "  188                 model.21.m.0.cv2.bn.bias     False          288                [288]     -0.597      0.527 torch.float32\n",
            "  189             model.21.m.1.cv1.conv.weight     False       746496     [288, 288, 3, 3]  -0.000224    0.00269 torch.float32\n",
            "  190               model.21.m.1.cv1.bn.weight     False          288                [288]       1.09      0.119 torch.float32\n",
            "  191                 model.21.m.1.cv1.bn.bias     False          288                [288]     -0.502       0.44 torch.float32\n",
            "  192             model.21.m.1.cv2.conv.weight     False       746496     [288, 288, 3, 3]   -0.00014    0.00256 torch.float32\n",
            "  193               model.21.m.1.cv2.bn.weight     False          288                [288]       1.26       0.12 torch.float32\n",
            "  194                 model.21.m.1.cv2.bn.bias     False          288                [288]     -0.347      0.503 torch.float32\n",
            "  195             model.22.cv2.0.0.conv.weight     False       110592      [64, 192, 3, 3]  -0.000262    0.00671 torch.float32\n",
            "  196               model.22.cv2.0.0.bn.weight     False           64                 [64]       1.05      0.468 torch.float32\n",
            "  197                 model.22.cv2.0.0.bn.bias     False           64                 [64]     -0.117      0.914 torch.float32\n",
            "  198             model.22.cv2.0.1.conv.weight     False        36864       [64, 64, 3, 3]   -0.00013       0.01 torch.float32\n",
            "  199               model.22.cv2.0.1.bn.weight     False           64                 [64]       2.21      0.924 torch.float32\n",
            "  200                 model.22.cv2.0.1.bn.bias     False           64                 [64]      0.866      0.705 torch.float32\n",
            "  201                  model.22.cv2.0.2.weight     False         4096       [64, 64, 1, 1]   0.000536     0.0435 torch.float32\n",
            "  202                    model.22.cv2.0.2.bias     False           64                 [64]       1.21      0.936 torch.float32\n",
            "  203             model.22.cv2.1.0.conv.weight     False       221184      [64, 384, 3, 3]  -0.000143    0.00455 torch.float32\n",
            "  204               model.22.cv2.1.0.bn.weight     False           64                 [64]       1.18      0.457 torch.float32\n",
            "  205                 model.22.cv2.1.0.bn.bias     False           64                 [64]     -0.069       0.77 torch.float32\n",
            "  206             model.22.cv2.1.1.conv.weight     False        36864       [64, 64, 3, 3]   8.91e-05    0.00936 torch.float32\n",
            "  207               model.22.cv2.1.1.bn.weight     False           64                 [64]       2.49       0.67 torch.float32\n",
            "  208                 model.22.cv2.1.1.bn.bias     False           64                 [64]       1.04        0.6 torch.float32\n",
            "  209                  model.22.cv2.1.2.weight     False         4096       [64, 64, 1, 1]  -0.000121     0.0479 torch.float32\n",
            "  210                    model.22.cv2.1.2.bias     False           64                 [64]       1.03      0.973 torch.float32\n",
            "  211             model.22.cv2.2.0.conv.weight     False       331776      [64, 576, 3, 3]  -0.000119    0.00315 torch.float32\n",
            "  212               model.22.cv2.2.0.bn.weight     False           64                 [64]       1.32      0.282 torch.float32\n",
            "  213                 model.22.cv2.2.0.bn.bias     False           64                 [64]     -0.142      0.594 torch.float32\n",
            "  214             model.22.cv2.2.1.conv.weight     False        36864       [64, 64, 3, 3]   5.51e-05    0.00745 torch.float32\n",
            "  215               model.22.cv2.2.1.bn.weight     False           64                 [64]       3.11      0.505 torch.float32\n",
            "  216                 model.22.cv2.2.1.bn.bias     False           64                 [64]       1.36      0.741 torch.float32\n",
            "  217                  model.22.cv2.2.2.weight     False         4096       [64, 64, 1, 1]  -0.000655      0.045 torch.float32\n",
            "  218                    model.22.cv2.2.2.bias     False           64                 [64]      0.741       1.19 torch.float32\n",
            "  219             model.22.cv3.0.0.conv.weight     False       331776     [192, 192, 3, 3]  -0.000241    0.00433 torch.float32\n",
            "  220               model.22.cv3.0.0.bn.weight     False          192                [192]       0.88       0.29 torch.float32\n",
            "  221                 model.22.cv3.0.0.bn.bias     False          192                [192]      -0.39      0.815 torch.float32\n",
            "  222             model.22.cv3.0.1.conv.weight     False       331776     [192, 192, 3, 3]  -0.000349    0.00398 torch.float32\n",
            "  223               model.22.cv3.0.1.bn.weight     False          192                [192]       1.97      0.634 torch.float32\n",
            "  224                 model.22.cv3.0.1.bn.bias     False          192                [192]      0.861       1.76 torch.float32\n",
            "  225                  model.22.cv3.0.2.weight     False         2112      [11, 192, 1, 1]   -0.00186     0.0416 torch.float32\n",
            "  226                    model.22.cv3.0.2.bias     False           11                 [11]      -11.1      0.768 torch.float32\n",
            "  227             model.22.cv3.1.0.conv.weight     False       663552     [192, 384, 3, 3]   -0.00017      0.003 torch.float32\n",
            "  228               model.22.cv3.1.0.bn.weight     False          192                [192]       1.05      0.292 torch.float32\n",
            "  229                 model.22.cv3.1.0.bn.bias     False          192                [192]      -0.28      0.936 torch.float32\n",
            "  230             model.22.cv3.1.1.conv.weight     False       331776     [192, 192, 3, 3]  -0.000428     0.0035 torch.float32\n",
            "  231               model.22.cv3.1.1.bn.weight     False          192                [192]       2.08      0.705 torch.float32\n",
            "  232                 model.22.cv3.1.1.bn.bias     False          192                [192]       0.75       1.47 torch.float32\n",
            "  233                  model.22.cv3.1.2.weight     False         2112      [11, 192, 1, 1]    -0.0023      0.043 torch.float32\n",
            "  234                    model.22.cv3.1.2.bias     False           11                 [11]      -8.99      0.571 torch.float32\n",
            "  235             model.22.cv3.2.0.conv.weight     False       995328     [192, 576, 3, 3]  -7.94e-05    0.00213 torch.float32\n",
            "  236               model.22.cv3.2.0.bn.weight     False          192                [192]        1.1       0.23 torch.float32\n",
            "  237                 model.22.cv3.2.0.bn.bias     False          192                [192]     -0.378      0.892 torch.float32\n",
            "  238             model.22.cv3.2.1.conv.weight     False       331776     [192, 192, 3, 3]  -0.000393    0.00286 torch.float32\n",
            "  239               model.22.cv3.2.1.bn.weight     False          192                [192]        2.1       0.47 torch.float32\n",
            "  240                 model.22.cv3.2.1.bn.bias     False          192                [192]      0.605       1.54 torch.float32\n",
            "  241                  model.22.cv3.2.2.weight     False         2112      [11, 192, 1, 1]   -0.00301     0.0426 torch.float32\n",
            "  242                    model.22.cv3.2.2.bias     False           11                 [11]      -7.47      0.401 torch.float32\n",
            "  243                 model.22.dfl.conv.weight     False           16        [1, 16, 1, 1]        7.5       4.76 torch.float32\n",
            "Model summary: 295 layers, 25862689 parameters, 0 gradients, 79.1 GFLOPs\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(295, 25862689, 0, 79.09785600000001)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Change the argument based on your experimental\n",
        "model = YOLO('../model/best.pt')\n",
        "model.info(detailed=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRG6Taq3MDgg",
        "outputId": "72a78fc3-cc06-4f2d-c7c1-6a6a09dfbc17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.210 🚀 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=last.pt, data=./data.yaml, epochs=100, patience=10, batch=16, imgsz=512, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=testing, exist_ok=False, pretrained=True, optimizer=Adamax, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.5, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=1e-05, lrf=0.01, momentum=0.99, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/testing\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
            "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
            "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
            "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
            "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
            "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
            "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
            "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
            "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
            " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
            " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
            " 22        [15, 18, 21]  1   3782065  ultralytics.nn.modules.head.Detect           [11, [192, 384, 576]]         \n",
            "Model summary: 295 layers, 25862689 parameters, 25862673 gradients, 79.1 GFLOPs\n",
            "\n",
            "Transferred 475/475 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/testing', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/train/labels.cache... 3584 images, 0 backgrounds, 0 corrupt: 100%|██████████| 3584/3584 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/val/labels.cache... 448 images, 0 backgrounds, 0 corrupt: 100%|██████████| 448/448 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/testing/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m Adamax(lr=1e-05, momentum=0.99) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
            "Image sizes 512 train, 512 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/testing\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      1/100        11G     0.7647     0.5859       1.17         54        512: 100%|██████████| 224/224 [01:31<00:00,  2.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.24it/s]\n",
            "                   all        448        761      0.863      0.837      0.904      0.682\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      2/100      10.6G     0.6967      0.488       1.12         59        512: 100%|██████████| 224/224 [01:25<00:00,  2.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.16it/s]\n",
            "                   all        448        761      0.865      0.883      0.915      0.697\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      3/100      10.6G     0.6704     0.4587      1.108         45        512: 100%|██████████| 224/224 [01:24<00:00,  2.66it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.19it/s]\n",
            "                   all        448        761      0.873      0.868      0.915      0.698\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      4/100      10.6G     0.6455     0.4331      1.092         71        512: 100%|██████████| 224/224 [01:24<00:00,  2.66it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.33it/s]\n",
            "                   all        448        761      0.875      0.882      0.918        0.7\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      5/100      10.6G     0.6541     0.4385      1.098         85        512: 100%|██████████| 224/224 [01:24<00:00,  2.66it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.69it/s]\n",
            "                   all        448        761      0.871      0.893       0.92        0.7\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      6/100      10.6G     0.6314     0.4136      1.087         47        512: 100%|██████████| 224/224 [01:23<00:00,  2.69it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.27it/s]\n",
            "                   all        448        761      0.864      0.897      0.921      0.702\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      7/100      10.6G     0.6232      0.408      1.079         72        512: 100%|██████████| 224/224 [01:23<00:00,  2.69it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.15it/s]\n",
            "                   all        448        761      0.862      0.894      0.918      0.704\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      8/100      10.6G     0.6282     0.4069      1.088         51        512: 100%|██████████| 224/224 [01:23<00:00,  2.69it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.11it/s]\n",
            "                   all        448        761      0.865      0.891      0.921      0.702\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      9/100      10.6G     0.6193     0.3955      1.077         65        512: 100%|██████████| 224/224 [01:23<00:00,  2.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.21it/s]\n",
            "                   all        448        761      0.869      0.896      0.921      0.699\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     10/100      10.6G     0.6204     0.4013      1.085         46        512: 100%|██████████| 224/224 [01:24<00:00,  2.66it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.43it/s]\n",
            "                   all        448        761        0.9      0.871      0.922      0.703\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     11/100      10.6G     0.6065     0.3921      1.074         50        512: 100%|██████████| 224/224 [01:24<00:00,  2.65it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.74it/s]\n",
            "                   all        448        761      0.895      0.876      0.923      0.705\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     12/100      10.6G     0.6121     0.3906      1.076         56        512: 100%|██████████| 224/224 [01:23<00:00,  2.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.16it/s]\n",
            "                   all        448        761      0.901      0.874      0.923      0.703\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     13/100      10.6G     0.6042     0.3873      1.074         61        512: 100%|██████████| 224/224 [01:24<00:00,  2.66it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.50it/s]\n",
            "                   all        448        761      0.881      0.892      0.922      0.701\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     14/100      10.6G     0.6063     0.3866       1.07         61        512: 100%|██████████| 224/224 [01:24<00:00,  2.64it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.77it/s]\n",
            "                   all        448        761      0.903      0.865      0.922      0.704\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     15/100      10.6G      0.604     0.3857      1.074         54        512: 100%|██████████| 224/224 [01:25<00:00,  2.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.66it/s]\n",
            "                   all        448        761      0.894      0.871      0.919      0.702\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     16/100      10.6G     0.5911     0.3773      1.066         55        512: 100%|██████████| 224/224 [01:26<00:00,  2.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.72it/s]\n",
            "                   all        448        761       0.88       0.89      0.918        0.7\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     17/100      10.6G     0.5985     0.3803      1.071         54        512: 100%|██████████| 224/224 [01:25<00:00,  2.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.29it/s]\n",
            "                   all        448        761      0.879      0.891      0.918      0.699\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     18/100      10.6G     0.5995     0.3864      1.071         52        512: 100%|██████████| 224/224 [01:23<00:00,  2.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.11it/s]\n",
            "                   all        448        761      0.878      0.894      0.922      0.703\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     19/100      10.6G     0.5977     0.3804      1.071         60        512: 100%|██████████| 224/224 [01:24<00:00,  2.65it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.24it/s]\n",
            "                   all        448        761      0.881      0.897      0.922      0.705\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     20/100      10.6G     0.5885     0.3748      1.067         60        512: 100%|██████████| 224/224 [01:23<00:00,  2.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.64it/s]\n",
            "                   all        448        761      0.882      0.899      0.926      0.708\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     21/100      10.6G     0.5886     0.3745      1.067         56        512: 100%|██████████| 224/224 [01:25<00:00,  2.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.42it/s]\n",
            "                   all        448        761       0.89      0.892      0.924      0.707\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     22/100      10.6G     0.5865     0.3747      1.063         53        512: 100%|██████████| 224/224 [01:23<00:00,  2.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.13it/s]\n",
            "                   all        448        761      0.889      0.882       0.92      0.705\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     23/100      10.6G     0.5756     0.3688      1.059         63        512: 100%|██████████| 224/224 [01:23<00:00,  2.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.10it/s]\n",
            "                   all        448        761        0.9      0.871      0.924      0.705\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     24/100      10.6G     0.5855       0.37      1.066         42        512: 100%|██████████| 224/224 [01:23<00:00,  2.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.30it/s]\n",
            "                   all        448        761      0.871      0.899      0.923      0.708\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     25/100      10.6G     0.5919     0.3782      1.066         51        512: 100%|██████████| 224/224 [01:23<00:00,  2.69it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.53it/s]\n",
            "                   all        448        761      0.883       0.89      0.924      0.709\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     26/100      10.6G     0.5851     0.3726      1.064         62        512: 100%|██████████| 224/224 [01:24<00:00,  2.64it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.17it/s]\n",
            "                   all        448        761      0.881      0.892      0.923      0.706\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     27/100      10.6G     0.5768     0.3652      1.059         48        512: 100%|██████████| 224/224 [01:23<00:00,  2.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.45it/s]\n",
            "                   all        448        761      0.884      0.893      0.921      0.703\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     28/100      10.6G     0.5761      0.368      1.056         62        512: 100%|██████████| 224/224 [01:24<00:00,  2.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.78it/s]\n",
            "                   all        448        761      0.883      0.906      0.923      0.702\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     29/100      10.6G      0.572     0.3602      1.058         53        512: 100%|██████████| 224/224 [01:25<00:00,  2.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.79it/s]\n",
            "                   all        448        761      0.889      0.895      0.922      0.707\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     30/100      10.6G     0.5704     0.3579      1.052         49        512: 100%|██████████| 224/224 [01:24<00:00,  2.66it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.39it/s]\n",
            "                   all        448        761      0.883      0.903      0.923      0.702\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     31/100      10.6G     0.5678     0.3564      1.053         44        512: 100%|██████████| 224/224 [01:23<00:00,  2.69it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:06<00:00,  2.13it/s]\n",
            "                   all        448        761      0.888      0.899      0.923      0.706\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     32/100      10.6G     0.5603     0.3561      1.052         73        512: 100%|██████████| 224/224 [01:23<00:00,  2.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.42it/s]\n",
            "                   all        448        761      0.892       0.89      0.924      0.708\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     33/100      10.6G     0.5677     0.3566      1.054         58        512: 100%|██████████| 224/224 [01:23<00:00,  2.68it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:04<00:00,  2.80it/s]\n",
            "                   all        448        761      0.897      0.882      0.924      0.707\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     34/100      10.6G     0.5785     0.3661      1.059         49        512: 100%|██████████| 224/224 [01:24<00:00,  2.66it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:04<00:00,  2.82it/s]\n",
            "                   all        448        761      0.896      0.888      0.923      0.707\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     35/100      10.6G     0.5543     0.3552      1.048         64        512: 100%|██████████| 224/224 [01:23<00:00,  2.68it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:05<00:00,  2.36it/s]\n",
            "                   all        448        761      0.902      0.882      0.924      0.705\n",
            "Stopping training early as no improvement observed in last 10 epochs. Best results observed at epoch 25, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
            "\n",
            "35 epochs completed in 0.903 hours.\n",
            "Optimizer stripped from runs/detect/testing/weights/last.pt, 52.0MB\n",
            "Optimizer stripped from runs/detect/testing/weights/best.pt, 52.0MB\n",
            "\n",
            "Validating runs/detect/testing/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.210 🚀 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 218 layers, 25846129 parameters, 0 gradients, 78.7 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:09<00:00,  1.53it/s]\n",
            "                   all        448        761      0.898      0.876      0.925      0.709\n",
            "                 apple        448         89      0.911      0.923      0.978      0.833\n",
            "               avocado        448        104      0.945      0.913      0.982      0.846\n",
            "                  cake        448         39      0.893      0.923      0.929      0.747\n",
            "        cooked chicken        448        137      0.623      0.591      0.622      0.376\n",
            "           dragonfruit        448        125      0.952      0.944      0.976      0.624\n",
            "          french fries        448         73      0.929      0.892       0.93       0.62\n",
            "            fried rice        448         50      0.925       0.98      0.982      0.723\n",
            "             hamburger        448         46      0.902      0.913      0.968      0.745\n",
            "               noodles        448         33      0.931      0.817      0.913      0.705\n",
            "              omelette        448         35      0.933      0.798       0.91      0.779\n",
            "                 salad        448         30      0.934       0.94      0.981      0.806\n",
            "Speed: 0.2ms preprocess, 7.2ms inference, 0.0ms loss, 2.5ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/testing\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7c692a1f50f0>\n",
              "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
              "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,     0.51149,     0.51149,           0],\n",
              "       [          1,           1,           1, ...,     0.11468,    0.057341,           0],\n",
              "       [          1,           1,           1, ...,    0.015205,   0.0076023,           0],\n",
              "       ...,\n",
              "       [          1,           1,           1, ...,    0.011253,   0.0056265,           0],\n",
              "       [          1,           1,           1, ...,      0.0123,   0.0061498,           0],\n",
              "       [          1,           1,           1, ...,     0.78947,     0.78947,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.65441,     0.65441,     0.74879, ...,           0,           0,           0],\n",
              "       [     0.7079,      0.7079,     0.79306, ...,           0,           0,           0],\n",
              "       [    0.55224,     0.55224,     0.71219, ...,           0,           0,           0],\n",
              "       ...,\n",
              "       [        0.5,         0.5,     0.63793, ...,           0,           0,           0],\n",
              "       [    0.51163,     0.51163,     0.61382, ...,           0,           0,           0],\n",
              "       [    0.51282,     0.51282,     0.60211, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.48634,     0.48634,     0.60255, ...,           1,           1,           1],\n",
              "       [     0.5508,      0.5508,     0.66566, ...,           1,           1,           1],\n",
              "       [    0.38947,     0.38947,     0.57006, ...,           1,           1,           1],\n",
              "       ...,\n",
              "       [    0.34066,     0.34066,     0.48295, ...,           1,           1,           1],\n",
              "       [    0.35106,     0.35106,     0.45503, ...,           1,           1,           1],\n",
              "       [    0.34483,     0.34483,     0.43073, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,     0.98876, ...,           0,           0,           0],\n",
              "       [    0.99038,     0.99038,     0.98077, ...,           0,           0,           0],\n",
              "       [    0.94872,     0.94872,     0.94872, ...,           0,           0,           0],\n",
              "       ...,\n",
              "       [    0.93939,     0.93939,     0.93939, ...,           0,           0,           0],\n",
              "       [    0.94286,     0.94286,     0.94286, ...,           0,           0,           0],\n",
              "       [          1,           1,           1, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
              "fitness: 0.7309565148042491\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([    0.83293,      0.8458,     0.74668,     0.37592,     0.62394,     0.62008,     0.72345,     0.74456,     0.70526,     0.77947,     0.80575])\n",
              "names: {0: 'apple', 1: 'avocado', 2: 'cake', 3: 'cooked chicken', 4: 'dragonfruit', 5: 'french fries', 6: 'fried rice', 7: 'hamburger', 8: 'noodles', 9: 'omelette', 10: 'salad'}\n",
              "plot: True\n",
              "results_dict: {'metrics/precision(B)': 0.8979524762310461, 'metrics/recall(B)': 0.8758476032897712, 'metrics/mAP50(B)': 0.9245993479209283, 'metrics/mAP50-95(B)': 0.7094406444579514, 'fitness': 0.7309565148042491}\n",
              "save_dir: PosixPath('runs/detect/testing')\n",
              "speed: {'preprocess': 0.16554657902036393, 'inference': 7.24094094974654, 'loss': 0.0010063605649130686, 'postprocess': 2.5291559951645985}\n",
              "task: 'detect'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.train(\n",
        "    data='./data.yaml',\n",
        "    epochs=100,\n",
        "    batch=16,\n",
        "    imgsz=512,\n",
        "    patience=10,\n",
        "    optimizer='Adamax',\n",
        "    lr0=1e-5,\n",
        "    close_mosaic=0,\n",
        "    dropout=0.5,\n",
        "    name='training',\n",
        "    cos_lr=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2VBjSN-XLpxN",
        "outputId": "37bce5d4-2891-4f6a-c035-aec2487a8d02"
      },
      "outputs": [],
      "source": [
        "shutil.make_archive('trained_model', 'zip', 'training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ80xr6o7wPp",
        "outputId": "7ad99607-aee4-40f7-915f-0039b640cc33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/test.jpg: 352x512 1 cooked chicken, 2 french friess, 1 hamburger, 1 noodles, 100.7ms\n",
            "Speed: 15.6ms preprocess, 100.7ms inference, 33.4ms postprocess per image at shape (1, 3, 352, 512)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "res = model('test.jpg', imgsz=512, save=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I9Y6jIEIGOh2",
        "outputId": "32ffc6dd-ecaa-470d-8423-d300eab67492"
      },
      "outputs": [],
      "source": [
        "model.export(format='onnx', imgsz=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pDD4djA6-9Xx",
        "outputId": "ce869d47-0bf9-4712-b625-ea08e8d60f4d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/food-detection-model.zip'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shutil.make_archive('food-detection-model', 'zip', './best_saved_model/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcbHx7uRTL93",
        "outputId": "8e4c768b-435b-46d2-f38e-2d363076dd95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: resize_nearest_neighbor (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.image.resize(...method=ResizeMethod.NEAREST_NEIGHBOR...)` instead.\n",
            "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to onnx_tf_prefix__model_22_sub_x, onnx_tf_prefix__model_22_mul_2_y in the SavedModel.\n",
            "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
            "INFO:absl:Writing fingerprint to tfjs_model/fingerprint.pb\n"
          ]
        }
      ],
      "source": [
        "onnx_model = onnx.load(\"best.onnx\")\n",
        "tf_rep = prepare(onnx_model)\n",
        "tf_rep.export_graph('tfjs_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKrpZi8TTyvm",
        "outputId": "e9f16bf2-be58-4c04-8a58-a15c63057c1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-12-07 12:48:27.191228: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-07 12:48:27.191280: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-07 12:48:27.191318: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-07 12:48:28.207810: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "WARNING:root:TensorFlow Decision Forests 1.8.1 is compatible with the following TensorFlow Versions: ['2.15.0']. However, TensorFlow 2.14.0 was detected. This can cause issues with the TF API and symbols in the custom C++ ops. See the TF and TF-DF compatibility table at https://github.com/tensorflow/decision-forests/blob/main/documentation/known_issues.md#compatibility-table.\n",
            "2023-12-07 12:48:33.505150: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "weight PartitionedCall/SelectV2_77 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_24 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_107 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_29 with shape (3,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_19 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_48 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_78 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_106 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_17 with shape (3,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_65 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_19 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_97 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_22 with shape (3,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_66 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_96 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_16 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_40 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_98 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight unknown_177 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Where_3 with shape (0, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_43 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/GatherV2_3 with shape (0,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_21 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight unknown_179 with shape (3,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Where_4 with shape (0, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_44 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/GatherV2_4 with shape (0,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_22 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_73 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_23 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_105 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_28 with shape (3,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_18 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_46 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_74 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_104 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_16 with shape (3,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Where with shape (0, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_32 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/GatherV2 with shape (0,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_14 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Where_1 with shape (0, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_37 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/GatherV2_1 with shape (0,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_17 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_57 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_15 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_83 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_16 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_14 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_34 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_58 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_82 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_14 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_49 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_12 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_69 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_12 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_12 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_29 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_50 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_68 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_12 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_41 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_10 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_56 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_10 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_10 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_25 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_42 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_55 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_10 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_45 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_11 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_58 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_11 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_11 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_27 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_46 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_57 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_11 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_33 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_8 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_47 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_8 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_8 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_19 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_34 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_46 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_8 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_37 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_9 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_49 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_9 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_9 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_21 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_38 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_48 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_9 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_53 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_13 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_71 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_13 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_13 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_31 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_54 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_70 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_13 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_25 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_6 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_38 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_6 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_6 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_13 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_26 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_37 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_6 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_17 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_4 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_25 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_4 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_4 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_9 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_18 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_24 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_4 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_9 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_2 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_12 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_2 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_2 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_5 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_10 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_11 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_2 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_1 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_3 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_1 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_2 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_2 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_5 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_1 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_5 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_1 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_1 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_3 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_6 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_4 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_1 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_13 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_3 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_14 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_3 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_3 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_7 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_14 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_13 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_3 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_21 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_5 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_27 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_5 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_5 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_11 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_22 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_26 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_5 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_29 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_7 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_40 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_7 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_7 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_15 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_30 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_39 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_7 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_61 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_16 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_85 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_17 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_15 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_36 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_62 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_84 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ones_15 with shape (4,) and dtype int64 was auto converted to the type int32\n",
            "weight unknown_128 with shape (3,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Where_2 with shape (0, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_38 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/GatherV2_2 with shape (0,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_18 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_69 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/zeros_20 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_100 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Shape_23 with shape (3,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/SelectV2_70 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_99 with shape () and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/ExpandDims_17 with shape (1, 1) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Cast_42 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight unknown_5 with shape (1,) and dtype int64 was auto converted to the type int32\n",
            "weight PartitionedCall/Const_101 with shape () and dtype int64 was auto converted to the type int32\n"
          ]
        }
      ],
      "source": [
        "!tensorflowjs_converter \\\n",
        "    --input_format=tf_saved_model \\\n",
        "    --output_format=tfjs_graph_model \\\n",
        "    --signature_name=serving_default \\\n",
        "    --saved_model_tags=serve \\\n",
        "    ./tfjs_model \\\n",
        "    ./food_detection_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oUYcJTx9UzeQ",
        "outputId": "8a7421ec-4147-417b-9b49-b2aafb4a32fb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/food_detection_model.zip'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shutil.make_archive('food_detection_model', 'zip', 'food_detection_model')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
